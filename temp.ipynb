{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(\"GRO620 - Problématique\")\n",
    "print(\"OpenCV version\", cv.__version__)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faec1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "\n",
    "images_fn = os.listdir(\"photos_prob/\")\n",
    "images_fn.sort()\n",
    "print(\"%i photo(s) à traiter\" % (len(images_fn)))\n",
    "if len(images_fn) == 0:\n",
    "    print(\n",
    "        \"ERREUR! Vérifiez que vous avez bien un dossier photos_prob au même endroit que ce calepin.\"\n",
    "    )\n",
    "\n",
    "images: list[cv.typing.MatLike] = []\n",
    "\n",
    "for f in images_fn:\n",
    "    img_path = os.path.join(\"photos_prob/\", f)\n",
    "    img = cv.imread(img_path)\n",
    "    assert img is not None\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    images.append(img)\n",
    "\n",
    "plt.figure(0)\n",
    "for index, image in enumerate(images):\n",
    "    plt.subplot(3, 3, index + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Original {index + 1}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc895a90",
   "metadata": {},
   "source": [
    "# Paramètres Intrinsèques\n",
    "\n",
    "Pour déterminer les valeurs $f_x$ et $f_y$ de la matrice $K$, on utilise les équivalences suivantes:\n",
    "\n",
    "$$ \\frac{f}{f_x} = \\frac{largeur_{capteur}}{largeur_{image}} $$\n",
    "$$ \\frac{f}{f_y} = \\frac{hauteur_{capteur}}{hauteur_{image}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parametres de la camera\n",
    "\n",
    "FOCAL_LENGTH = 23.0e-3  # m\n",
    "IMG_WIDTH = 640  # px\n",
    "IMG_HEIGHT = 427  # px\n",
    "SENSOR_WIDTH = 23.4e-3  # m\n",
    "SENSOR_HEIGHT = 15.6e-3  # m\n",
    "\n",
    "# Calcul des parametres intrinseques\n",
    "fx = FOCAL_LENGTH * (IMG_WIDTH / SENSOR_WIDTH)  # px\n",
    "fy = FOCAL_LENGTH * (IMG_HEIGHT / SENSOR_HEIGHT)  # px\n",
    "cx = IMG_WIDTH / 2.0  # px\n",
    "cy = IMG_HEIGHT / 2.0  # px\n",
    "skew = 0.0  # px\n",
    "\n",
    "K_tilde = np.array(\n",
    "    [\n",
    "        [fx, skew, cx, 0],\n",
    "        [0, fy, cy, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "print(f\"Matrice intrinseque K:\\n{K_tilde}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d7e01",
   "metadata": {},
   "source": [
    "# Matrices de transformation et de projection\n",
    "\n",
    "À partir de l'équation 2.64 du manuel de cours, on dérive l'équation pour calculer la matrice de projection en coordonnées homogènes:\n",
    "\n",
    "$$ \\tilde{P} = \\tilde{T}_{c}\\cdot\\tilde{K}^{-1}, $$\n",
    "\n",
    "où $\\tilde{T}_{c}$ est la matrice de transformation en coordonnées homogènes du repère $\\{C\\}$ au repère $\\{0\\}$.\n",
    "\n",
    "Puisque le plan $XY$ du repère $\\{0\\}$ est coplanaire avec le convoyeur, on dérive la distance verticale entre le convoyeur et la caméra:\n",
    "\n",
    "$$ z_{c} \\equiv z_{s} = \\tilde{T}_{c, \\{3, 4\\}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Matrice de transformation extrinseque w_T_c (repere {C} au repere {0})\n",
    "\n",
    "w_T_c_tilde = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0.500],\n",
    "        [0, -1, 0, 0.200],\n",
    "        [0, 0, -1, 0.282],\n",
    "        [0, 0, 0, 1],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "print(f\"Matrice extrinseque w_T_c (Camera -> World):\\n{w_T_c_tilde}\\n\")\n",
    "\n",
    "# Matrice de projection complete\n",
    "\n",
    "w_P_c_tilde = w_T_c_tilde @ np.linalg.inv(K_tilde)\n",
    "\n",
    "print(f\"Matrice de projection complete (Camera -> World):\\n{w_P_c_tilde}\\n\")\n",
    "\n",
    "# Calcul de z_c\n",
    "X_0 = 0  # m\n",
    "Y_0 = 0  # m\n",
    "Z_0 = 0  # m\n",
    "z_conveyor = (\n",
    "    w_T_c_tilde[2, 3]\n",
    "    + w_T_c_tilde[2, 0] * X_0\n",
    "    + w_T_c_tilde[2, 1] * Y_0\n",
    "    + w_T_c_tilde[2, 3] * Z_0\n",
    ")  # m\n",
    "\n",
    "print(f\"z_c: {z_conveyor:.3f} m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59add425",
   "metadata": {},
   "source": [
    "# Traitement de l'image et extraction des caractéristiques\n",
    "\n",
    "Dans les images à traiter, on observe certaines particularités:\n",
    "\n",
    "- Les objets à identifier possèdent une couleur distincte du reste de l'image;\n",
    "- Les objets à identifier possèdent une forme similaire;\n",
    "- L'arrière-plan (convoyeur) n'est pas uniforme, et représente du bruit.\n",
    "\n",
    "On cherche à identifier et caractériser les vis dans les images. On sépare le problème en deux volets:\n",
    "\n",
    "1. Isoler les vis dans l'image;\n",
    "1. Déterminer la pose des vis dans l'image.\n",
    "\n",
    "L'isolation des vis dans les images se fait par une chaîne de traitement d'image. Les paramètres de chaque étape de la chaîne sont déterminés de façon expérimentale (*i.e.I essai-erreur). La chaîne d'acquisition proposée implémente la logique suivante:\n",
    "\n",
    "1. Réduire le bruit de l'arrière-plan, de sorte à l'uniformiser;\n",
    "1. Appliquer un masque binaire à l'image afin de ségréger les vis et l'arrière-plan;\n",
    "1. Appliquer des opérations morphologiques afin d'éliminer le bruit résiduel, et assurer que la forme originale des vis soit préservée (nécessaire pour une détection de contours idéale).\n",
    "\n",
    "Pour déterminer la pose des vis dans l'image, une seconde chaîne de traitement implémente la logique suivante:\n",
    "\n",
    "1. Détecter les contours des vis;\n",
    "1. Filter les contours résultant du bruit résiduel;\n",
    "1. Trouver la boîte englobante de chaque vis afin d'extraire leurs caractéristiques de position, de dimensions, et d'orientation;\n",
    "1. Standardiser la notation des caractéristiques des vis;\n",
    "1. Classifier les vis en fonction de leurs dimensions;\n",
    "1. Convertir la position 2D dans l'image en coordonnées 3D.\n",
    "\n",
    "Additionnellement, on dessine les boîtes englobantes (accompagnées de leur identifiant respectif) sur l'image originale afin de vérifier visuellement les résultats.\n",
    "\n",
    "L'équation de conversion en coordonnées 3D est dérivée de l'équation 2.67, ainsi que des équations du document \"Transformation 2D à 3D\" fourni dans le cadre de la problématique.\n",
    "\n",
    "## Note sur le calcul de l'orientation\n",
    "\n",
    "Le calcul de l'orientation des vis est effectué à partir de la boîte englobante. Toutefois, tel qu'observé dans les images annotées, l'orientation des boîtes diffère un peu de l'orientation des vis: il y a donc une certaine tolérance associée aux valeurs de l'orientation (de l'ordre de quelques degrés).\n",
    "\n",
    "Une stratégie qui résulterait en des valeurs plus justes serait d'utiliser la transformée de Hough pour identifier les lignes \"directrices\" dans l'image, qui sont parallèles aux vis. Cette stratégie possède aussi des limites: plusieurs lignes peuvent appartenir à la même vis. Les résultats de la transformée nécessite donc d'être agglomérer (*clustering*). Tel qu'observé dans le code ci-dessous, les paramètres de l'algorithme d'agglomération qui permettent d'obtenir un ratio $1:1$ (c.à.d. une ligne par vis) n'ont pas été identifiés. Considérant que l'agglomération des lignes est en dehors de la portée du cours, les résultats produits par l'algorithme d'agglomération ne sont pas utilisés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster._hdbscan.hdbscan import HDBSCAN\n",
    "\n",
    "processed = []\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for index, image in enumerate(images):\n",
    "    original = image.copy()\n",
    "    blurred = cv.GaussianBlur(original, (7, 7), 3.0)\n",
    "    gray = cv.cvtColor(blurred, cv.COLOR_RGB2GRAY)\n",
    "    _, threshold = cv.threshold(gray, 180, 255, cv.THRESH_BINARY)\n",
    "    kernel = np.ones((3, 3), dtype=np.uint8)\n",
    "    opening = cv.morphologyEx(threshold, cv.MORPH_OPEN, kernel)\n",
    "    dilated = cv.dilate(opening, kernel, iterations=1)\n",
    "    processed.append(dilated)\n",
    "\n",
    "    lines = cv.HoughLinesP(\n",
    "        dilated, 2.0, np.pi / 180, 100, minLineLength=25, maxLineGap=5\n",
    "    )\n",
    "\n",
    "    # Cluster lines detected by Hough transform\n",
    "    # NOTE: ideally, we want one line per screw, but this is rather difficult to obtain with clustering.\n",
    "    midpoints = np.array(\n",
    "        [\n",
    "            [(line[0][0] + line[0][2]) / 2, (line[0][1] + line[0][3]) / 2]  # type: ignore\n",
    "            for line in lines\n",
    "        ]\n",
    "    )\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=2\n",
    "    )  # Large-ish `n` samples, medium `n` clusters, cluster by distance betweem nearest points\n",
    "    labels = clusterer.fit_predict(midpoints)\n",
    "    # Average lines for each cluster label\n",
    "    clustered_lines = []\n",
    "    for lbl in np.unique(labels):\n",
    "        if lbl == -1:\n",
    "            continue  # skip noise\n",
    "        idxs = np.where(labels == lbl)[0]\n",
    "        # Average the endpoints of all lines in this cluster\n",
    "        avg_line = np.mean([lines[i][0] for i in idxs], axis=0)\n",
    "        clustered_lines.append(avg_line.astype(int))\n",
    "    lines = np.array(clustered_lines).reshape(-1, 1, 4)\n",
    "\n",
    "    # print(f\"Image {index + 1}: {len(lines)} lines detected\")\n",
    "\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        theta = np.rad2deg(np.atan2(y2 - y1, x2 - x1))\n",
    "        cv.line(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Identify, classify, and compute pose\n",
    "    contours, _ = cv.findContours(dilated, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    MIN_SCREW_AREA = 100  # px\n",
    "    MAX_SCREW_AREA = 2000  # px\n",
    "    HEIGHT_THRESHOLD = 100.0  # px\n",
    "\n",
    "    # Extract caracterstics\n",
    "    results = []\n",
    "    screw_id: int = 0\n",
    "    for contour in contours:\n",
    "        area = cv.contourArea(contour)\n",
    "        if area < MIN_SCREW_AREA or area > MAX_SCREW_AREA:\n",
    "            continue\n",
    "\n",
    "        rect = cv.minAreaRect(contour)\n",
    "        (c_u, c_v), (width, height), theta = rect\n",
    "\n",
    "        # Draw bounding box and id\n",
    "        bbox = cv.boxPoints(rect)\n",
    "        bbox = np.intp(bbox)  # Convert box points to integer type for drawing\n",
    "        cv.drawContours(image, [bbox], 0, (0, 0, 255), 2)  # type: ignore\n",
    "        cv.putText(\n",
    "            image,\n",
    "            str(screw_id),\n",
    "            (int(c_u) - 5, int(c_v) - 5),\n",
    "            cv.FONT_HERSHEY_SIMPLEX,\n",
    "            1.0,\n",
    "            (255, 0, 0),\n",
    "            3,\n",
    "            cv.LINE_AA,\n",
    "        )\n",
    "\n",
    "        # Standardise notation\n",
    "        if width > height:\n",
    "            width, height = height, width\n",
    "            theta += 90\n",
    "        theta = (theta + 90) % 180\n",
    "        theta = (360 - theta) % 180\n",
    "\n",
    "        screw_type = \"courte\"\n",
    "        if height > HEIGHT_THRESHOLD:\n",
    "            screw_type = \"longue\"\n",
    "\n",
    "        # # px -> m ({C} frame)\n",
    "        z_screen = z_conveyor  # px?\n",
    "        x_screen = c_u * z_screen  # px^2?\n",
    "        y_screen = c_v * z_screen  # px^2?\n",
    "        X_s_prime = np.array([x_screen, y_screen, z_screen, 1], dtype=np.float32)\n",
    "\n",
    "        # # {C} -> {0}\n",
    "        p_w = w_P_c_tilde @ X_s_prime\n",
    "        x_0, y_0, z_0, _ = p_w\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"id\": screw_id,\n",
    "                \"type\": screw_type,\n",
    "                \"X\": x_0,\n",
    "                \"Y\": y_0,\n",
    "                \"Z\": z_0,\n",
    "                \"theta\": theta,\n",
    "            }\n",
    "        )\n",
    "        screw_id += 1\n",
    "\n",
    "    all_results.append(results)\n",
    "\n",
    "plt.figure(0)\n",
    "for index, image in enumerate(processed):\n",
    "    plt.subplot(3, 3, index + 1)\n",
    "    plt.imshow(image, \"gray\")\n",
    "    plt.title(f\"Image {index + 1}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cd79a",
   "metadata": {},
   "source": [
    "# Analyse des résultats\n",
    "\n",
    "Les faiblesses suivantes de la chaîne de traitement ont été identifiées:\n",
    "\n",
    "- L'orientation des vis est indépendante de la tête, et la précision n'est pas idéale (la tolérance est de l'ordre de $\\pm 10\\degree$);\n",
    "- La chaîne de traitement n'a pas été évaluée dans une situation différente. Une image où des débris s'entremêlent avec les vis pourrait résulter en une performance médiocre. Autrement dit, la chaîne de traitement n'est pas robuste.\n",
    "\n",
    "Un moyen de mitiger l'erreur sur l'orientation des vis serait d'augmenter l'ouverture du préhenseur du robot manipulateur afin d'assurer le succès d'une préhension sur l'entièreté de la plage de valeurs possibles.\n",
    "\n",
    "La force principale de la chaîne de traitement identifiée est: \"ça fonctionne!\".\n",
    "\n",
    "Considérant que seulement une vis n'a pas été détectée (voir 'image 5'), et que la majorité de cette vis était à l'extérieur du champ de vision de la caméra (donc manque d'information pour extraire ses caractéristiques), et considérant que quelques boîtes englobantes n'englobent pas complètement leur vis respective, mais que l'algorithme de classification identifie correctement le type de la vis, et que l'erreur en position est de l'ordre de quelques pixels, la performance de la chaîne de traitement est jugé suffisante pour répondre aux besoins du problème posé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce147272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "\n",
    "import tabulate\n",
    "\n",
    "for index, _ in enumerate(images):\n",
    "    print(f\"Image {index + 1}:\\n\")\n",
    "    print(\n",
    "        tabulate.tabulate(\n",
    "            [\n",
    "                [\n",
    "                    screw[\"id\"],\n",
    "                    screw[\"type\"],\n",
    "                    screw[\"X\"],\n",
    "                    screw[\"Y\"],\n",
    "                    screw[\"Z\"],\n",
    "                    screw[\"theta\"],\n",
    "                ]\n",
    "                for screw in all_results[index]\n",
    "            ],\n",
    "            headers=[\n",
    "                \"id\",\n",
    "                \"Type\",\n",
    "                \"X (m)\",\n",
    "                \"Y (m)\",\n",
    "                \"Z (m)\",\n",
    "                \"theta (deg)\",\n",
    "            ],\n",
    "            floatfmt=\".3f\",\n",
    "        )\n",
    "    )\n",
    "    print()\n",
    "\n",
    "plt.figure()\n",
    "for index, image in enumerate(images):\n",
    "    plt.subplot(3, 3, index + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Image {index + 1}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
